{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: Loss = 20.577748631067788\n",
      "Layer 5: Loss = 20.54441529456309\n",
      "Layer 10: Loss = 20.511081958070886\n",
      "Layer 15: Loss = 20.477748621592486\n",
      "Layer 20: Loss = 20.444415285129345\n",
      "Layer 25: Loss = 20.411081948683062\n",
      "Layer 30: Loss = 20.377748612255406\n",
      "Layer 35: Loss = 20.34441527584835\n",
      "Layer 40: Loss = 20.311081939464046\n",
      "Layer 45: Loss = 20.277748603104893\n",
      "Layer 50: Loss = 20.24441526677354\n",
      "Layer 55: Loss = 20.21108193047289\n",
      "Layer 60: Loss = 20.177748594206193\n",
      "Layer 65: Loss = 20.14441525797702\n",
      "Layer 70: Loss = 20.111081921789303\n",
      "Layer 75: Loss = 20.077748585647402\n",
      "Layer 80: Loss = 20.044415249556142\n",
      "Layer 85: Loss = 20.011081913520844\n",
      "Layer 90: Loss = 19.9777485775474\n",
      "Layer 95: Loss = 19.9444152416423\n",
      "Layer 100: Loss = 19.91108190581274\n",
      "Layer 105: Loss = 19.87774857006666\n",
      "Layer 110: Loss = 19.84441523441284\n",
      "Layer 115: Loss = 19.81108189886098\n",
      "Layer 120: Loss = 19.777748563421802\n",
      "Layer 125: Loss = 19.744415228107155\n",
      "Layer 130: Loss = 19.711081892930125\n",
      "Layer 135: Loss = 19.6777485579052\n",
      "Layer 140: Loss = 19.64441522304836\n",
      "Layer 145: Loss = 19.61108188837729\n",
      "Layer 150: Loss = 19.57774855391152\n",
      "Layer 155: Loss = 19.54441521967263\n",
      "Layer 160: Loss = 19.51108188568449\n",
      "Layer 165: Loss = 19.477748551973466\n",
      "Layer 170: Loss = 19.444415218568693\n",
      "Layer 175: Loss = 19.411081885502384\n",
      "Layer 180: Loss = 19.377748552810118\n",
      "Layer 185: Loss = 19.34441522053124\n",
      "Layer 190: Loss = 19.311081888709214\n",
      "Layer 195: Loss = 19.27774855739209\n",
      "Layer 200: Loss = 19.24441522663295\n",
      "Layer 205: Loss = 19.211081896490487\n",
      "Layer 210: Loss = 19.177748567029543\n",
      "Layer 215: Loss = 19.14441523832179\n",
      "Layer 220: Loss = 19.111081910446433\n",
      "Layer 225: Loss = 19.07774858349101\n",
      "Layer 230: Loss = 19.04441525755226\n",
      "Layer 235: Loss = 19.01108193273711\n",
      "Layer 240: Loss = 18.9777486091637\n",
      "Layer 245: Loss = 18.94441528696265\n",
      "Layer 250: Loss = 18.911081966278275\n",
      "Layer 255: Loss = 18.877748647270078\n",
      "Layer 260: Loss = 18.844415330114337\n",
      "Layer 265: Loss = 18.811082015005884\n",
      "Layer 270: Loss = 18.77774870216003\n",
      "Layer 275: Loss = 18.744415391814744\n",
      "Layer 280: Loss = 18.71108208423301\n",
      "Layer 285: Loss = 18.677748779705507\n",
      "Layer 290: Loss = 18.644415478553466\n",
      "Layer 295: Loss = 18.611082181131916\n",
      "Layer 300: Loss = 18.577748887833238\n",
      "Layer 305: Loss = 18.5444155990911\n",
      "Layer 310: Loss = 18.51108231538478\n",
      "Layer 315: Loss = 18.477749037244013\n",
      "Layer 320: Loss = 18.444415765254238\n",
      "Layer 325: Loss = 18.411082500062538\n",
      "Layer 330: Loss = 18.37774924238406\n",
      "Layer 335: Loss = 18.344415993009232\n",
      "Layer 340: Loss = 18.354432348756355\n",
      "Layer 345: Loss = 18.38776555779193\n",
      "Layer 350: Loss = 18.421098754103415\n",
      "Layer 355: Loss = 18.45443193635327\n",
      "Layer 360: Loss = 18.48776510306344\n",
      "Layer 365: Loss = 18.52109825260062\n",
      "Layer 370: Loss = 18.554431383159958\n",
      "Layer 375: Loss = 18.58776449274706\n",
      "Layer 380: Loss = 18.62109757915815\n",
      "Layer 385: Loss = 18.654430639958086\n",
      "Layer 390: Loss = 18.68776367245614\n",
      "Layer 395: Loss = 18.72109667367923\n",
      "Layer 400: Loss = 18.754429640342362\n",
      "Layer 405: Loss = 18.78776256881598\n",
      "Layer 410: Loss = 18.821095455089928\n",
      "Layer 415: Loss = 18.85442829473366\n",
      "Layer 420: Loss = 18.88776108285232\n",
      "Layer 425: Loss = 18.92109381403824\n",
      "Layer 430: Loss = 18.954426482317423\n",
      "Layer 435: Loss = 18.987759081090463\n",
      "Layer 440: Loss = 19.02109160306735\n",
      "Layer 445: Loss = 19.054424040195546\n",
      "Layer 450: Loss = 19.08775638358055\n",
      "Layer 455: Loss = 19.12108862339834\n",
      "Layer 460: Loss = 19.154420748798703\n",
      "Layer 465: Loss = 19.1877527477986\n",
      "Layer 470: Loss = 19.221084607164524\n",
      "Layer 475: Loss = 19.25441631228264\n",
      "Layer 480: Loss = 19.2877478470156\n",
      "Layer 485: Loss = 19.32107919354443\n",
      "Layer 490: Loss = 19.35441033219407\n",
      "Layer 495: Loss = 19.387741241240757\n",
      "Final Result is: [73.78260453 11.78433677  8.7550504 ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Original linear model: y = b + wx (Too simple to use)\n",
    "Sigmoid function model: Suppose we have three sigmoid function\n",
    "\n",
    "\"\"\"\n",
    "# Viewers for previous three days: X = [25, 20, 21]\n",
    "# There are weight for no. of sigmoid and no. of features. So Wij represents weight for xj for i-th sigmoid\n",
    "# Bias: different bias for different functions\n",
    "\n",
    "\"\"\"\n",
    "r1 = b1 + w11x1 + w12x2 + w13x3\n",
    "r2 = b2 + w21x1 + w22x2 + w23x3\n",
    "r3 = b3 + w31x1 + w32x2 + w33x3\n",
    "\n",
    "Where \n",
    "result vector: R = [r1, r2, r3]\n",
    "weight matrix W = [[w11, w12, w13], [w21, w22, w23], [w31, w32, w33]]\n",
    "input vector X = [x1, x2, x3]\n",
    "bias vector B = [b1, b2, b3]\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#### --------------------------------------- Define Model -------------------------------------- ####\n",
    "# Define the sigmoid function\n",
    "def sigmoid(r):\n",
    "    return 1 / (1 + np.exp(-r))\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the input vector\n",
    "original_x = np.array([33, 28, 32])\n",
    "\n",
    "def model(X, W, B, C, B2):\n",
    "    # Compute the result\n",
    "    R = np.dot(W, X) + B\n",
    "\n",
    "    # Apply the sigmoid function to each element of R\n",
    "    sigmoid_R = sigmoid(R)\n",
    "\n",
    "    # Scale by coefficients\n",
    "    sigmoid_R_scaled = C * sigmoid_R\n",
    "\n",
    "    # Compute the final output\n",
    "    Y = B2 + sigmoid_R_scaled\n",
    "    \n",
    "    return Y\n",
    "\n",
    "def deep_learn(original_data, layers, learning_rate=0.01):\n",
    "    # Initialize weights, biases, and coefficients\n",
    "    W = 2 * np.random.rand(3, 3) - 1\n",
    "    B = 50 + 10 * np.random.rand(3)\n",
    "    C = np.random.rand(3) * 100 + 0.5\n",
    "    B2 = 0.1 * np.random.rand(3)\n",
    "    \n",
    "    for layer in range(layers):\n",
    "        # Compute output\n",
    "        output = model(original_data, W, B, C, B2)\n",
    "        \n",
    "        # Assume the watched amount for next day is [53, 46, 12]\n",
    "        real_watch = np.array([53, 46, 12])\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(real_watch, output)\n",
    "\n",
    "        # Compute gradients\n",
    "        error = output - real_watch  # (predicted - actual)\n",
    "        d_sigmoid = sigmoid(output) * (1 - sigmoid(output))  # derivative of sigmoid\n",
    "\n",
    "        # Gradients\n",
    "        dC = sigmoid(output)\n",
    "        dB2 = np.ones_like(B2)\n",
    "        dW = np.outer(error * d_sigmoid, original_data)  # gradients for W\n",
    "        dB = error * d_sigmoid  # gradients for B\n",
    "\n",
    "        # Update parameters\n",
    "        W -= learning_rate * dW\n",
    "        B -= learning_rate * dB\n",
    "        C -= learning_rate * dC\n",
    "        B2 -= learning_rate * dB2\n",
    "        \n",
    "        if layer % 5 == 0:  # Print loss every 50 layers\n",
    "            print(f\"Layer {layer}: Loss = {loss}\")\n",
    "\n",
    "    return output\n",
    "\n",
    "def loss_function(real_values: list, predict_values: list):\n",
    "    real = np.array(real_values)\n",
    "    predict = np.array(predict_values)\n",
    "\n",
    "    # Compute the absolute errors\n",
    "    absolute_errors = np.abs(real - predict)\n",
    "\n",
    "    # Sum the absolute errors\n",
    "    total_error = np.sum(absolute_errors)\n",
    "    \n",
    "    # Calculate MAE by dividing by the number of items\n",
    "    mae = total_error / len(real)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# Execute the training\n",
    "final_output = deep_learn(original_x, layers=500, learning_rate=0.01)\n",
    "print(\"Final Result is:\", final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
