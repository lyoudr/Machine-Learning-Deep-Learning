{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Distance: 0.4170105755329132\n",
      "Document: And what of Tom Lefroy? He returned to London and in 1796 became engaged to a Miss Mary Paul, the sister of a friend; they married two years later. He rose through the ranks to become Lord Chief Justice of Ireland. Years later, when asked about Jane Austen, he said that he had loved her, although he qualified this by saying it was ‘a boyish love’.\n",
      "\n",
      "Result 2:\n",
      "Distance: 0.42943987250328064\n",
      "Document: Jane Austen famously never married, but she did have admirers. The best known of these is Tom Lefroy, a clever young Irishman whom she met in December 1795. He had moved to London to study law and was spending the Christmas holidays with his uncle and aunt who lived in Ashe, near Steventon.\n",
      "\n",
      "Jane had just turned 20 and was a bright, lively, pretty girl. Like her much-loved heroine Elizabeth Bennet, she enjoyed music and dancing, wit, laughter and lively conversation.\n",
      "\n",
      "Result 3:\n",
      "Distance: 0.43448787927627563\n",
      "Document: Although Tom only stayed in Hampshire for a few weeks, the pair met frequently at Christmas balls and parties. They danced, chatted and flirted, until Tom went back to London to resume his studies. Jane went back to work too – it was around then that she began writing First Impressions (later published as Pride and Prejudice).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fl/0bw4y3r54p9_9zf1xp7_wr900000gn/T/ipykernel_15952/100488082.py:64: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
      "/var/folders/fl/0bw4y3r54p9_9zf1xp7_wr900000gn/T/ipykernel_15952/100488082.py:78: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 Response: Tom Lefroy is a clever young Irishman who moved to London to study law. He is known as one of Jane Austen's admirers. He later became engaged to Miss Mary Paul and eventually rose through the ranks to become Lord Chief Justice of Ireland.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA \n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = None\n",
    "\n",
    "# * 1. Convert document into embeddings (vectors) using an embedding model.\n",
    "# * 2. Store vectors in a vector database (FAISS, ChromaDB, Pinecone, etc.).\n",
    "# * 3. Perform similarity search: Retrieve top-k similar vector and return their indices and distances.\n",
    "# * 4. Pass retrieved context into LLM (e.g., GPT-4) via a prompt.\n",
    "\n",
    "# Load text document\n",
    "loader = TextLoader(\"data.txt\")\n",
    "documents = loader.load() \n",
    "\n",
    "# Split the document into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Convert document chunks to vector embeddings\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Define the query text\n",
    "query = \"Who is Tom ?\"\n",
    "\n",
    "# Convert query into an embedding vector\n",
    "query_vector = embeddings.embed_query(query)\n",
    "\n",
    "# Perform similarity search\n",
    "k = 3 # Number of top results to retrieve\n",
    "distances, indices = vector_store.index.search(\n",
    "    np.array([query_vector]), k\n",
    ")\n",
    "\n",
    "# Retrieve matched documents\n",
    "retrieved_docs = [docs[i] for i in indices[0]]\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"Distance: {distances[0][i]}\")\n",
    "    print(f\"Document: {doc.page_content}\\n\")\n",
    "\"\"\"\n",
    "1. The query is embedded into a high-dimensional vector\n",
    "2. FAISS performs a similarity search and returns:\n",
    "    (1) indices: The indexes of the closest documents\n",
    "    (2) distances: The similarity scores (lower = more similar).\n",
    "3. We retrieve the corresponding documents and display their distances.\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Pass Retrieved Chunks into LLM Prompt\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize GPT-4 model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Format retrieved documents into a single string\n",
    "retrieved_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Construct the prompt with retrieved context\n",
    "prompt = f\"\"\"\n",
    "Use the following retrieved information to answer the query:\n",
    "{retrieved_text}\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Generate response using GPT-4\n",
    "response = llm.predict(prompt)\n",
    "print(\"GPT-4 Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
